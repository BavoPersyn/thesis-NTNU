@InProceedings{WesterhoffLessmannMeuterSiegemundKummert2016,
  author    = {Westerhoff, Jens and Lessmann, Stephanie and Meuter, Mirko and Siegemund, Jan and Kummert, Anton},
  booktitle = {2016 IEEE Intelligent Vehicles Symposium (IV)},
  title     = {Development and comparison of homography based estimation techniques for camera to road surface orientation},
  year      = {2016},
  month     = {June},
  pages     = {1034-1040},
  abstract  = {This paper focuses on dynamic orientation estimation of a vehicle-mounted mono camera. In particular, the pitch and roll angles of the camera relative to the road surface. Information about the orientation angles is included in the homography (projective transformation) between two images of a planar road surface. The extraction of angles from a homography matrix is possible but not recommended due to parameter ambiguities. For this reason we do not estimate a full homography matrix but reduce the parameter space to two parameters. In this area of parameter estimation there are mainly two different approaches: The optical flow based approach and the image registration based approach. In order to decide which of these approaches is more favorable for the angle estimation problem, we develop one optical flow based and one image registration based angle estimation algorithm. We are the first directly evaluating and comparing both approaches with each other with the help of an artificial image sequence as well as real-world driving scenarios. In addition, this paper lifts the common limitation of roll angle of zero degree for dynamic camera orientation estimation. Our research finds that there is only a small difference between the parameter estimation results of the optical flow and image registration based approach.},
  doi       = {10.1109/IVS.2016.7535516},
}

@Article{6096039,
  author   = {Scaramuzza, Davide and Fraundorfer, Friedrich},
  journal  = {IEEE Robotics Automation Magazine},
  title    = {Visual Odometry [Tutorial]},
  year     = {2011},
  issn     = {1558-223X},
  month    = {Dec},
  number   = {4},
  pages    = {80-92},
  volume   = {18},
  abstract = {Visual odometry (VO) is the process of estimating the egomotion of an agent (e.g., vehicle, human, and robot) using only the input of a single or If multiple cameras attached to it. Application domains include robotics, wearable computing, augmented reality, and automotive. The term VO was coined in 2004 by Nister in his landmark paper. The term was chosen for its similarity to wheel odometry, which incrementally estimates the motion of a vehicle by integrating the number of turns of its wheels over time. Likewise, VO operates by incrementally estimating the pose of the vehicle through examination of the changes that motion induces on the images of its onboard cameras. For VO to work effectively, there should be sufficient illumination in the environment and a static scene with enough texture to allow apparent motion to be extracted. Furthermore, consecutive frames should be captured by ensuring that they have sufficient scene overlap.},
  doi      = {10.1109/MRA.2011.943233},
}

@Article{6153423,
  author   = {Fraundorfer, Friedrich and Scaramuzza, Davide},
  journal  = {IEEE Robotics Automation Magazine},
  title    = {Visual Odometry : Part II: Matching, Robustness, Optimization, and Applications},
  year     = {2012},
  issn     = {1558-223X},
  month    = {June},
  number   = {2},
  pages    = {78-90},
  volume   = {19},
  abstract = {Part II of the tutorial has summarized the remaining building blocks of the VO pipeline: specifically, how to detect and match salient and repeatable features across frames and robust estimation in the presence of outliers and bundle adjustment. In addition, error propagation, applications, and links to publicly available code are included. VO is a well understood and established part of robotics. VO has reached a maturity that has allowed us to successfully use it for certain classes of applications: space, ground, aerial, and underwater. In the presence of loop closures, VO can be used as a building block for a complete SLAM algorithm to reduce motion drift. Challenges that still remain are to develop and demonstrate large-scale and long-term implementations, such as driving autonomous cars for hundreds of miles. Such systems have recently been demonstrated using Lidar and Radar sensors [86]. However, for VO to be used in such systems, technical issues regarding robustness and, especially, long-term stability have to be resolved. Eventually, VO has the potential to replace Lidar-based systems for egomotion estimation, which are currently leading the state of the art in accuracy, robustness, and reliability. VO offers a cheaper and mechanically easier-to-manufacture solution for egomotion estimation, while, additionally, being fully passive. Furthermore, the ongoing miniaturization of digital cameras offers the possibility to develop smaller and smaller robotic systems capable of ego-motion estimation.},
  doi      = {10.1109/MRA.2012.2182810},
}

@InProceedings{6126544,
    author      = {Rublee, Ethan and Rabaud, Vincent and Konolige, Kurt and Bradski, Gary},  
    booktitle   = {2011 International Conference on Computer Vision},   
    title       = {ORB: An efficient alternative to SIFT or SURF},   
    year        = {2011},  
    volume      = {},  
    number      = {},  
    pages       = {2564-2571},  
    doi         = {10.1109/ICCV.2011.6126544}
}

@book{book,
author = {Trucco, Emanuele and Verri, Alessandro},
year = {1998},
month = {01},
pages = {},
title = {Introductory techniques for 3-D computer vision.},
isbn = {978-0-13-261108-4}
}

@InProceedings{10.1007/11744023_34,
author="Rosten, Edward
and Drummond, Tom",
editor="Leonardis, Ale{\v{s}}
and Bischof, Horst
and Pinz, Axel",
title="Machine Learning for High-Speed Corner Detection",
booktitle="Computer Vision -- ECCV 2006",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="430--443",
abstract="Where feature points are used in real-time frame-rate applications, a high-speed feature detector is necessary. Feature detectors such as SIFT (DoG), Harris and SUSAN are good methods which yield high quality features, however they are too computationally intensive for use in real-time applications of any complexity. Here we show that machine learning can be used to derive a feature detector which can fully process live PAL video using less than 7{\%} of the available processing time. By comparison neither the Harris detector (120{\%}) nor the detection stage of SIFT (300{\%}) can operate at full frame rate.",
isbn="978-3-540-33833-8"
}


@Comment{jabref-meta: databaseType:bibtex;}
